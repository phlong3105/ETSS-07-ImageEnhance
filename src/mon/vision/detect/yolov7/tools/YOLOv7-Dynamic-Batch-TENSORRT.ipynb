{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1ddbda-adf0-4ff0-b104-8b519123ae59",
   "metadata": {},
   "source": [
    "import sys\n",
    "import torch\n",
    "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
    "print(f\"Pytorch version: {torch.__version__} \")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c5ef37-56cd-47ed-917c-b4cf606963bf",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e92ace9-9366-426b-99ca-44de8dc55b1c",
   "metadata": {},
   "source": [
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e34dad-2269-4f96-a78c-c887bb0d3495",
   "metadata": {},
   "source": [
    "!python detect.py --weights ./yolov7-tiny.pt --conf 0.25 --img-size 640 --source inference/images"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8c15bd-e55b-4dc1-b9f9-e04a398f85bd",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "Image.open('runs/detect/exp/bus.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59466ff9-b4d6-4b4f-9e40-ca7b84041b3f",
   "metadata": {},
   "source": [
    "Image.open('runs/detect/exp/horses.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc6030c-d233-41e3-9002-993a3ba0a55f",
   "metadata": {},
   "source": [
    "Image.open('runs/detect/exp/image1.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c59b96-7b91-41f9-8b5c-aa0e5fbc9364",
   "metadata": {},
   "source": [
    "Image.open('runs/detect/exp/image2.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b9aa536-bb44-47e4-a4ad-37a3ff6b760f",
   "metadata": {},
   "source": [
    "Image.open('runs/detect/exp/image3.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e55301-37ff-4214-97e4-f81a956c4269",
   "metadata": {},
   "source": [
    "Image.open('runs/detect/exp/zidane.jpg')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1569e541-7ebf-41c0-a7d0-7434a3618c98",
   "metadata": {},
   "source": [
    "# export temporary ONNX model for TensorRT converter\n",
    "!python export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --dynamic-batch\n",
    "!ls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac55007-f3ba-487b-9a15-853749debb59",
   "metadata": {},
   "source": [
    "# Download ONNX to TensorRT converter\n",
    "!git clone https://github.com/triple-Mu/YOLO-TensorRT8.git"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0efca310-9cce-4132-8cbf-5116845607c8",
   "metadata": {},
   "source": [
    "%cd YOLO-TensorRT8\n",
    "!ls"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee352a7-78a3-4cfd-8bf5-008fd4245625",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Export TensorRT-engine model \n",
    "!python build_engine.py -o ../yolov7-tiny.onnx -e ./yolov7-tiny-nms.trt --fp16 --batch-size 1 16 32"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35c88c4-18bd-429f-b700-732eccbccfaa",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict,namedtuple"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c0a670-5bc1-4dcd-8460-4e3dafab57bb",
   "metadata": {},
   "source": [
    "w = './yolov7-tiny-nms.trt'\n",
    "device = torch.device('cuda:0')\n",
    "imgList = [cv2.imread('../inference/images/horses.jpg'),\n",
    "           cv2.imread('../inference/images/bus.jpg'),\n",
    "           cv2.imread('../inference/images/zidane.jpg'),\n",
    "           cv2.imread('../inference/images/image1.jpg'),\n",
    "           cv2.imread('../inference/images/image2.jpg'),\n",
    "           cv2.imread('../inference/images/image3.jpg')]\n",
    "imgList*=6\n",
    "imgList = imgList[:32]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b5b62e0-be22-463e-b961-46baca126bd2",
   "metadata": {},
   "source": [
    "# Infer TensorRT Engine\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "trt.init_libnvinfer_plugins(logger, namespace=\"\")\n",
    "with open(w, 'rb') as f, trt.Runtime(logger) as runtime:\n",
    "    model = runtime.deserialize_cuda_engine(f.read())\n",
    "context = model.create_execution_context()\n",
    "\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "def postprocess(boxes,r,dwdh):\n",
    "    dwdh = torch.tensor(dwdh*2).to(boxes.device)\n",
    "    boxes -= dwdh\n",
    "    boxes /= r\n",
    "    return boxes.clip_(0,6400)\n",
    "\n",
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', \n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', \n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', \n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', \n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', \n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', \n",
    "         'hair drier', 'toothbrush']\n",
    "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c71646-e978-40db-8478-5c4329a058ec",
   "metadata": {},
   "source": [
    "origin_RGB = []\n",
    "resize_data = []\n",
    "for img in imgList:\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  origin_RGB.append(img)\n",
    "  image = img.copy()\n",
    "  image, ratio, dwdh = letterbox(image, auto=False)\n",
    "  image = image.transpose((2, 0, 1))\n",
    "  image = np.expand_dims(image, 0)\n",
    "  image = np.ascontiguousarray(image)\n",
    "  im = image.astype(np.float32)\n",
    "  resize_data.append((im,ratio,dwdh))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b87a7e55-b0f3-498f-b261-a6954e662494",
   "metadata": {},
   "source": [
    "DTYPE = {\n",
    "    trt.DataType.FLOAT : torch.float32,\n",
    "    trt.DataType.INT32 : torch.int32,\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e5a4e80-aaf0-4b6f-b155-83442aee088e",
   "metadata": {},
   "source": [
    "def getBindings(model,context,shape=(1,3,640,640)):\n",
    "    context.set_binding_shape(0, shape)\n",
    "    bindings = OrderedDict()\n",
    "    Binding = namedtuple('Binding', ('name', 'dtype', 'shape', 'data', 'ptr'))\n",
    "    \n",
    "    for index in range(model.num_bindings):\n",
    "        name = model.get_binding_name(index)\n",
    "        dtype = trt.nptype(model.get_binding_dtype(index))\n",
    "        shape = tuple(context.get_binding_shape(index))\n",
    "        data = torch.from_numpy(np.empty(shape, dtype=np.dtype(dtype))).to(device)\n",
    "        bindings[name] = Binding(name, dtype, shape, data, int(data.data_ptr()))\n",
    "    return bindings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98f8f0d8-5940-4f72-9ffb-88dc4aa09c09",
   "metadata": {},
   "source": [
    "# warmup for 10 times\n",
    "bindings = getBindings(model,context,(4,3,640,640))\n",
    "binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n",
    "for _ in range(10):\n",
    "    tmp = torch.randn(4,3,640,640).to(device)\n",
    "    binding_addrs['images'] = int(tmp.data_ptr())\n",
    "    context.execute_v2(list(binding_addrs.values()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f24b5e-ac71-41e5-9893-067f75fb6457",
   "metadata": {},
   "source": [
    "np_batch = np.concatenate([data[0] for data in resize_data])\n",
    "np_batch.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbaa433b-1b04-4bdf-a81d-ca7ba683dc25",
   "metadata": {},
   "source": [
    "batch_1 = torch.from_numpy(np_batch[0:1]).to(device)/255\n",
    "bindings = getBindings(model,context,(1,3,640,640))\n",
    "binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n",
    "\n",
    "print(\"batch==1\")\n",
    "start = time.perf_counter()\n",
    "binding_addrs['images'] = int(batch_1.data_ptr())\n",
    "context.execute_v2(list(binding_addrs.values()))\n",
    "print(f'Cost {time.perf_counter()-start} s')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c364a3d9-405e-4b8b-b5fc-1ca24b6cb45d",
   "metadata": {},
   "source": [
    "batch_16 = torch.from_numpy(np_batch[0:16]).to(device)/255\n",
    "bindings = getBindings(model,context,(16,3,640,640))\n",
    "binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n",
    "\n",
    "print(\"batch==16\")\n",
    "start = time.perf_counter()\n",
    "binding_addrs['images'] = int(batch_16.data_ptr())\n",
    "context.execute_v2(list(binding_addrs.values()))\n",
    "print(f'Cost {time.perf_counter()-start} s')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab73d41-e63f-4458-aea9-85dc0907589d",
   "metadata": {},
   "source": [
    "batch_32 = torch.from_numpy(np_batch[0:32]).to(device)/255\n",
    "bindings = getBindings(model,context,(32,3,640,640))\n",
    "binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n",
    "\n",
    "print(\"batch==32\")\n",
    "start = time.perf_counter()\n",
    "binding_addrs['images'] = int(batch_32.data_ptr())\n",
    "context.execute_v2(list(binding_addrs.values()))\n",
    "print(f'Cost {time.perf_counter()-start} s')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b75c09f-3d54-44c8-9c8c-9198aa4513b0",
   "metadata": {},
   "source": [
    "# show batch 32 output the first 6 pictures\n",
    "nums = bindings['num_dets'].data\n",
    "boxes = bindings['det_boxes'].data\n",
    "scores = bindings['det_scores'].data\n",
    "classes = bindings['det_classes'].data\n",
    "nums.shape,boxes.shape,scores.shape,classes.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26676d29-8c1e-4e87-ac97-bff4c935b92a",
   "metadata": {},
   "source": [
    "for batch,(num,box,score,cls) in enumerate(zip(nums.flatten(),boxes,scores,classes)):\n",
    "    if batch>6:\n",
    "        break\n",
    "    RGB = origin_RGB[batch]\n",
    "    ratio,dwdh = resize_data[batch][1:]\n",
    "    box = postprocess(box[:num].clone(),ratio,dwdh).round().int()\n",
    "    for idx,(b,s,c) in enumerate(zip(box,score,cls)):\n",
    "        b,s,c = b.tolist(),round(float(s),3),int(c)\n",
    "        name = names[c]\n",
    "        color = colors[name]\n",
    "        name += ' ' + str(s)\n",
    "        cv2.rectangle(RGB,b[:2],b[2:],color,2)\n",
    "        cv2.putText(RGB,name,(b[0], b[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,color,thickness=2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19c7cc65-20c8-415f-96d9-8ad0be5aaf70",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83fe9bf8-6b34-4835-ad77-bc5d55a94251",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdf84a2d-9460-4046-92d0-7f164e0b9c9f",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62744e63-5e51-4b24-a534-13f1873cd37e",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfda8018-dc6e-40ef-9f4e-f2e85ad70512",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[4])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "286e50a2-9cb1-4d35-8f97-914af1c8883f",
   "metadata": {},
   "source": [
    "Image.fromarray(origin_RGB[5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14d60f-2f20-4266-b817-01425f99ee45",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
